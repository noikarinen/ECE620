{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a155f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from io import StringIO\n",
    "\n",
    "import arviz as az\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "\n",
    "from lifelines import KaplanMeierFitter, LogNormalFitter, WeibullFitter\n",
    "from lifelines.utils import survival_table_from_events\n",
    "from scipy.stats import binom, lognorm, norm, weibull_min\n",
    "import seaborn as sns\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "493d1558",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10343/671373809.py:53: UserWarning: The effect of Potentials on other parameters is ignored during prior predictive sampling. This is likely to lead to invalid or biased predictive samples.\n",
      "  idata = pm.sample_prior_predictive()\n",
      "Sampling: [alpha, beta, y_obs]\n",
      "Multiprocess sampling (32 chains in 32 jobs)\n",
      "NUTS: [beta, alpha]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='5133' class='' max='64000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      8.02% [5133/64000 00:01&lt;00:18 Sampling 32 chains, 0 divergences]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Not enough samples to build a trace.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 61\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mdict\u001b[39m\u001b[38;5;241m=\u001b[39m{}\n\u001b[1;32m     60\u001b[0m start_time_ns \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime_ns()\n\u001b[0;32m---> 61\u001b[0m idataN, modelN \u001b[38;5;241m=\u001b[39m make_model(priors,pm\u001b[38;5;241m.\u001b[39mNUTS)\n\u001b[1;32m     62\u001b[0m end_time_ns \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime_ns()\n\u001b[1;32m     63\u001b[0m timer_ns \u001b[38;5;241m=\u001b[39m end_time_ns \u001b[38;5;241m-\u001b[39m start_time_ns\n",
      "Cell \u001b[0;32mIn[2], line 54\u001b[0m, in \u001b[0;36mmake_model\u001b[0;34m(p, alg, info)\u001b[0m\n\u001b[1;32m     52\u001b[0m     y_cens \u001b[38;5;241m=\u001b[39m pm\u001b[38;5;241m.\u001b[39mPotential(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_cens\u001b[39m\u001b[38;5;124m\"\u001b[39m, weibull_lccdf(y[censored], alpha, beta))\n\u001b[1;32m     53\u001b[0m     idata \u001b[38;5;241m=\u001b[39m pm\u001b[38;5;241m.\u001b[39msample_prior_predictive()\n\u001b[0;32m---> 54\u001b[0m     idata\u001b[38;5;241m.\u001b[39mextend(pm\u001b[38;5;241m.\u001b[39msample(draws\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,tune\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, cores\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,step\u001b[38;5;241m=\u001b[39malg()))\n\u001b[1;32m     55\u001b[0m     idata\u001b[38;5;241m.\u001b[39mextend(pm\u001b[38;5;241m.\u001b[39msample_posterior_predictive(idata))\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m idata, model\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pymc/sampling/mcmc.py:827\u001b[0m, in \u001b[0;36msample\u001b[0;34m(draws, tune, chains, cores, random_seed, progressbar, step, nuts_sampler, initvals, init, jitter_max_retries, n_init, trace, discard_tuned_samples, compute_convergence_checks, keep_warning_stat, return_inferencedata, idata_kwargs, nuts_sampler_kwargs, callback, mp_ctx, model, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m t_sampling \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t_start\n\u001b[1;32m    825\u001b[0m \u001b[38;5;66;03m# Packaging, validating and returning the result was extracted\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;66;03m# into a function to make it easier to test and refactor.\u001b[39;00m\n\u001b[0;32m--> 827\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _sample_return(\n\u001b[1;32m    828\u001b[0m     run\u001b[38;5;241m=\u001b[39mrun,\n\u001b[1;32m    829\u001b[0m     traces\u001b[38;5;241m=\u001b[39mtraces,\n\u001b[1;32m    830\u001b[0m     tune\u001b[38;5;241m=\u001b[39mtune,\n\u001b[1;32m    831\u001b[0m     t_sampling\u001b[38;5;241m=\u001b[39mt_sampling,\n\u001b[1;32m    832\u001b[0m     discard_tuned_samples\u001b[38;5;241m=\u001b[39mdiscard_tuned_samples,\n\u001b[1;32m    833\u001b[0m     compute_convergence_checks\u001b[38;5;241m=\u001b[39mcompute_convergence_checks,\n\u001b[1;32m    834\u001b[0m     return_inferencedata\u001b[38;5;241m=\u001b[39mreturn_inferencedata,\n\u001b[1;32m    835\u001b[0m     keep_warning_stat\u001b[38;5;241m=\u001b[39mkeep_warning_stat,\n\u001b[1;32m    836\u001b[0m     idata_kwargs\u001b[38;5;241m=\u001b[39midata_kwargs \u001b[38;5;129;01mor\u001b[39;00m {},\n\u001b[1;32m    837\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    838\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pymc/sampling/mcmc.py:858\u001b[0m, in \u001b[0;36m_sample_return\u001b[0;34m(run, traces, tune, t_sampling, discard_tuned_samples, compute_convergence_checks, return_inferencedata, keep_warning_stat, idata_kwargs, model)\u001b[0m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;66;03m# Pick and slice chains to keep the maximum number of samples\u001b[39;00m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m discard_tuned_samples:\n\u001b[0;32m--> 858\u001b[0m     traces, length \u001b[38;5;241m=\u001b[39m _choose_chains(traces, tune)\n\u001b[1;32m    859\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    860\u001b[0m     traces, length \u001b[38;5;241m=\u001b[39m _choose_chains(traces, \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pymc/backends/base.py:601\u001b[0m, in \u001b[0;36m_choose_chains\u001b[0;34m(traces, tune)\u001b[0m\n\u001b[1;32m    599\u001b[0m lengths \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(trace) \u001b[38;5;241m-\u001b[39m tune) \u001b[38;5;28;01mfor\u001b[39;00m trace \u001b[38;5;129;01min\u001b[39;00m traces]\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(lengths):\n\u001b[0;32m--> 601\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot enough samples to build a trace.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    603\u001b[0m idxs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(lengths)\n\u001b[1;32m    604\u001b[0m l_sort \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(lengths)[idxs]\n",
      "\u001b[0;31mValueError\u001b[0m: Not enough samples to build a trace."
     ]
    }
   ],
   "source": [
    "\n",
    "y = np.array([56]*3000 + [117]*1500 + [266]*1250 + [315]*1150 + [405]*1100 + [501]*1000 +\n",
    "                 [615]*1100 + [704]*1150 + [838]*120 + [901]*130 + [1010]*125 + [1111]*100 +\n",
    "                 [1250]*50 + [1350]*400 + [1500]*300 + [1600]*100 + [1700]*5 + [1870]*3 +\n",
    "                 [2070]*4 + [1550]*1 + [1050]*1 + [1000]*1 + [500]*1 + [350]*1 + [250]*1)\n",
    "\n",
    "censored = np.array([True]*3000 + [True]*1500 + [True]*1250+ [True]*1150 + [True]*1100 + [True]*1000 +\n",
    "                 [True]*1100 + [True]*1150 + [True]*120 + [True]*130 + [True]*125 + [True]*100 +\n",
    "                 [True]*50 + [True]*400 + [True]*300 + [True]*100 + [True]*5 + [True]*3 +\n",
    "                 [True]*4 + [False]*1 + [False]*1 + [False]*1 + [False]*1 + [False]*1 + [False]*1)\n",
    "\n",
    "\n",
    "def weibull_lccdf(y, alpha, beta):\n",
    "    \"\"\"Log complementary cdf of Weibull distribution.\"\"\"\n",
    "    return -((y / beta) ** alpha)\n",
    "\n",
    "priors = {\"beta\": [100, 15000], \"alpha\": [4, 1, 0.02, 8]}\n",
    "priors_informative = {\"beta\": [10000, 500], \"alpha\": [2, 0.5, 0.02, 3]}\n",
    "\n",
    "def dict_master(Sampler_name, idata,timer_s,dict):\n",
    "  \n",
    "    \n",
    "    summary=az.summary(idata,kind='all')\n",
    "    \n",
    "    summary_dict=pd.DataFrame.to_dict(summary,orient=\"dict\")\n",
    "    dict.update({Sampler_name:summary_dict})   \n",
    "    summary_dict.update({\"Computation time(sec)\":timer_s})\n",
    "\n",
    "    dict.update(dict)\n",
    "    \n",
    "    \n",
    "    dict_update=dict\n",
    "\n",
    "    with open(\"ece620.json\", \"w\") as outfile:\n",
    "        json.dump(dict_update,outfile,indent=4)\n",
    "\n",
    "    return dict_update\n",
    "\n",
    "\n",
    "\n",
    "def make_model(p, alg, info=False):\n",
    "    with pm.Model() as model:\n",
    "        if info:\n",
    "            beta = pm.Normal(\"beta\", p[\"beta\"][0], p[\"beta\"][1])\n",
    "        else:\n",
    "            beta = pm.Uniform(\"beta\", p[\"beta\"][0], p[\"beta\"][1])\n",
    "        alpha = pm.TruncatedNormal(\n",
    "            \"alpha\", p[\"alpha\"][0], p[\"alpha\"][1], lower=p[\"alpha\"][2], upper=p[\"alpha\"][3]\n",
    "        )\n",
    "        \n",
    "        \n",
    "        y_obs = pm.Weibull(\"y_obs\", alpha=alpha, beta=beta, observed=y[~censored])\n",
    "        y_cens = pm.Potential(\"y_cens\", weibull_lccdf(y[censored], alpha, beta))\n",
    "        idata = pm.sample_prior_predictive()\n",
    "        idata.extend(pm.sample(draws=1000,tune=1000, cores=32,step=alg()))\n",
    "        idata.extend(pm.sample_posterior_predictive(idata))\n",
    "\n",
    "    return idata, model\n",
    "dict={}\n",
    "\n",
    "start_time_ns = time.time_ns()\n",
    "idataN, modelN = make_model(priors,pm.NUTS)\n",
    "end_time_ns = time.time_ns()\n",
    "timer_ns = end_time_ns - start_time_ns\n",
    "timer_s=timer_ns/1000000000\n",
    "print(timer_s,'seconds')\n",
    "\n",
    "dict = dict_master(\"NUTS\",idataN,timer_s,dict)\n",
    "\n",
    "\n",
    "start_time_ns = time.time_ns()\n",
    "idataM, modelM = make_model(priors,pm.Metropolis)\n",
    "end_time_ns = time.time_ns()\n",
    "timer_ns = end_time_ns - start_time_ns\n",
    "timer_s=timer_ns/1000000000\n",
    "print(timer_s,'seconds')\n",
    "\n",
    "dict = dict_master(\"Metropolis\",idataM,timer_s,dict)\n",
    "\n",
    "start_time_ns = time.time_ns()\n",
    "idataDEM, modelDEM = make_model(priors,pm.DEMetropolis)\n",
    "end_time_ns = time.time_ns()\n",
    "timer_ns = end_time_ns - start_time_ns\n",
    "timer_s=timer_ns/1000000000\n",
    "print(timer_s,'seconds')\n",
    "\n",
    "dict = dict_master(\"DEMetropolis\",idataDEM,timer_s,dict)\n",
    "\n",
    "start_time_ns = time.time_ns()\n",
    "idataDEMZ, modelDEMZ = make_model(priors,pm.DEMetropolisZ)\n",
    "end_time_ns = time.time_ns()\n",
    "timer_ns = end_time_ns - start_time_ns\n",
    "timer_s=timer_ns/1000000000\n",
    "print(timer_s,'seconds')\n",
    "\n",
    "dict = dict_master(\"DEMetropolisZ\",idataDEM,timer_s,dict)\n",
    "\n",
    "start_time_ns = time.time_ns()\n",
    "idataS, modelS = make_model(priors,pm.Slice)\n",
    "end_time_ns = time.time_ns()\n",
    "timer_ns = end_time_ns - start_time_ns\n",
    "timer_s=timer_ns/1000000000\n",
    "print(timer_s,'seconds')\n",
    "\n",
    "dict = dict_master(\"Slice\",idataS,timer_s,dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bb1e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(idataN);\n",
    "az.plot_trace(idataM);\n",
    "az.plot_trace(idataDEM);\n",
    "az.plot_trace(idataDEMZ);\n",
    "az.plot_trace(idataS);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2d44b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time_ns = time.time_ns()\n",
    "idata_informativeN, modelN = make_model(priors_informative,pm.NUTS, info=True)\n",
    "end_time_ns = time.time_ns()\n",
    "timer_ns = end_time_ns - start_time_ns\n",
    "timer_s=timer_ns/1000000000\n",
    "print(timer_s,'seconds')\n",
    "\n",
    "dict = dict_master(\"NUTS Informed\",idata_informativeN,timer_s,dict)\n",
    "\n",
    "\n",
    "start_time_ns = time.time_ns()\n",
    "idata_informativeM, modelM = make_model(priors_informative,pm.Metropolis, info=True)\n",
    "end_time_ns = time.time_ns()\n",
    "timer_ns = end_time_ns - start_time_ns\n",
    "timer_s=timer_ns/1000000000\n",
    "print(timer_s,'seconds')\n",
    "\n",
    "dict = dict_master(\"Metropolis Informed\",idata_informativeM,timer_s,dict)\n",
    "\n",
    "start_time_ns = time.time_ns()\n",
    "idata_informativeDEM, modelDEM = make_model(priors_informative,pm.DEMetropolis, info=True)\n",
    "end_time_ns = time.time_ns()\n",
    "timer_ns = end_time_ns - start_time_ns\n",
    "timer_s=timer_ns/1000000000\n",
    "print(timer_s,'seconds')\n",
    "\n",
    "dict = dict_master(\"DEMetropolis Informed\",idata_informativeDEM,timer_s,dict)\n",
    "\n",
    "start_time_ns = time.time_ns()\n",
    "idata_informativeDEMZ, modelDEMZ = make_model(priors_informative,pm.DEMetropolisZ, info=True)\n",
    "end_time_ns = time.time_ns()\n",
    "timer_ns = end_time_ns - start_time_ns\n",
    "timer_s=timer_ns/1000000000\n",
    "print(timer_s,'seconds')\n",
    "\n",
    "dict = dict_master(\"DEMetropolisZ Informed\",idata_informativeDEMZ,timer_s,dict)\n",
    "\n",
    "start_time_ns = time.time_ns()\n",
    "idata_informativeS, modelS = make_model(priors_informative,pm.Slice, info=True)\n",
    "end_time_ns = time.time_ns()\n",
    "timer_ns = end_time_ns - start_time_ns\n",
    "timer_s=timer_ns/1000000000\n",
    "print(timer_s,'seconds')\n",
    "\n",
    "dict = dict_master(\"Slice Informed\",idata_informativeS,timer_s,dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9836b2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(idata_informativeN);\n",
    "az.plot_trace(idata_informativeM);\n",
    "az.plot_trace(idata_informativeDEM);\n",
    "az.plot_trace(idata_informativeDEMZ);\n",
    "az.plot_trace(idata_informativeS);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3059b1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ecdf(sample):\n",
    "    # convert sample to a numpy array, if it isn't already\n",
    "    sample = np.atleast_1d(sample)\n",
    "\n",
    "    # find the unique values and their corresponding counts\n",
    "    quantiles, counts = np.unique(sample, return_counts=True)\n",
    "\n",
    "    # take the cumulative sum of the counts and divide by the sample size to\n",
    "    # get the cumulative probabilities between 0 and 1\n",
    "    cumprob = np.cumsum(counts).astype(np.double) / sample.size\n",
    "\n",
    "    return quantiles, cumprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9e4e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_draws = az.extract(idataNUTS, num_samples=1000)[[\"alpha\", \"beta\"]]\n",
    "alphas = joint_draws[\"alpha\"].values\n",
    "betas = joint_draws[\"beta\"].values\n",
    "\n",
    "joint_draws_informative = az.extract(idata_informativeNUTS, num_samples=1000)[[\"alpha\", \"beta\"]]\n",
    "alphas_informative = joint_draws_informative[\"alpha\"].values\n",
    "betas_informative = joint_draws_informative[\"beta\"].values\n",
    "\n",
    "draws = pm.draw(pm.Weibull.dist(alpha=np.mean(alphas), beta=np.mean(betas)), 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e239bcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "hist_data = []\n",
    "for i in range(1000):\n",
    "    draws = pm.draw(pm.Weibull.dist(alpha=alphas[i], beta=betas[i]), 1000)\n",
    "    qe, pe = ecdf(draws)\n",
    "    lkup = dict(zip(pe, qe))\n",
    "    hist_data.append([lkup[0.1], lkup[0.05]])\n",
    "    ax.plot(qe, pe, color=\"cyan\", alpha=0.1)\n",
    "\n",
    "hist_data_info = []\n",
    "for i in range(1000):\n",
    "    draws = pm.draw(pm.Weibull.dist(alpha=alphas_informative[i], beta=betas_informative[i]), 1000)\n",
    "    qe, pe = ecdf(draws)\n",
    "    lkup = dict(zip(pe, qe))\n",
    "    hist_data_info.append([lkup[0.1], lkup[0.05]])\n",
    "    ax.plot(qe, pe, color=\"pink\", alpha=0.1)\n",
    "\n",
    "hist_data = pd.DataFrame(hist_data, columns=[\"p10\", \"p05\"])\n",
    "hist_data_info = pd.DataFrame(hist_data_info, columns=[\"p10\", \"p05\"])\n",
    "\n",
    "draws = pm.draw(pm.Weibull.dist(alpha=np.mean(alphas), beta=np.mean(betas)), 1000)\n",
    "qe, pe = ecdf(draws)\n",
    "ax.plot(qe, pe, color=\"blue\", label=\"Expected CDF Uninformative Prior\")\n",
    "\n",
    "draws = pm.draw(pm.Weibull.dist(alpha=np.mean(alphas_informative), beta=np.mean(betas_informative)), 1000)\n",
    "qe, pe = ecdf(draws)\n",
    "ax.plot(qe, pe, color=\"red\", label=\"Expected CDF Informative Prior\")\n",
    "\n",
    "ax.set_xlim(0, 30000)\n",
    "ax.set_title(\"Bayesian Estimation of Uncertainty in the Posterior Predictive CDF \\n Informative and Non-Informative Priors\", fontsize=20)\n",
    "ax.set_ylabel(\"Fraction Failing\")\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796debb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "ax1.hist(hist_data[\"p10\"], bins=30, ec=\"black\", color=\"cyan\", alpha=0.4, label=\"Uninformative Prior\")\n",
    "ax1.hist(hist_data_info[\"p10\"], bins=30, ec=\"black\", color=\"red\", alpha=0.4, label=\"Informative Prior\")\n",
    "ax1.set_title(\"Distribution of 10% failure Time\", fontsize=20);\n",
    "ax1.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5aee35",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax2 = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "ax2.hist(hist_data[\"p05\"], bins=30, ec=\"black\", color=\"cyan\", alpha=0.4, label=\"Uninformative Prior\")\n",
    "ax2.hist(hist_data_info[\"p05\"], bins=30, ec=\"black\", color=\"red\", alpha=0.4, label=\"Informative Prior\")\n",
    "ax2.legend()\n",
    "ax2.set_title(\"Distribution of 5% failure Time\", fontsize=20);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025f5b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(idataNUTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b85a44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(idata_informativeNUTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc1786d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb227fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "\n",
    "from xarray_einstats.stats import XrContinuousRV, XrDiscreteRV\n",
    "\n",
    "\n",
    "def PI_failures(joint_draws, lp, up, n_at_risk):\n",
    "    fit = XrContinuousRV(weibull_min, joint_draws[\"alpha\"], scale=joint_draws[\"beta\"])\n",
    "    rho = fit.cdf(up) - fit.cdf(lp) / (1 - fit.cdf(lp))\n",
    "    lub = XrDiscreteRV(binom, n_at_risk, rho).ppf([0.05, 0.95])\n",
    "    lb, ub = lub.sel(quantile=0.05, drop=True), lub.sel(quantile=0.95, drop=True)\n",
    "    point_prediction = n_at_risk * rho\n",
    "    return xr.Dataset(\n",
    "        {\"rho\": rho, \"n_at_risk\": n_at_risk, \"lb\": lb, \"ub\": ub, \"expected\": point_prediction}\n",
    "    )\n",
    "\n",
    "\n",
    "output_ds = PI_failures(joint_draws, 150, 800, 1300)\n",
    "output_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384c72fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_func(failures, power):\n",
    "    #Imagined cost function for failing item e.g. refunds required\n",
    "    return np.power(failures, power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3d398c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.scatter(\n",
    "    joint_draws[\"alpha\"],\n",
    "    output_ds[\"expected\"],\n",
    "    c=joint_draws[\"beta\"],\n",
    "    cmap=cm.winter,\n",
    "    alpha=0.3,\n",
    "    label=\"Coloured by function of Beta values\",\n",
    ")\n",
    "ax.legend()\n",
    "ax.set_ylabel(\"Expected Failures\")\n",
    "ax.set_xlabel(\"Alpha\")\n",
    "ax.set_title(\n",
    "    \"Posterior Predictive Expected Failure Count between 100-500 hours \\nas a function of Weibull(alpha, beta)\",\n",
    "    fontsize=20,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142b84f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "ax1.hist(\n",
    "    output_ds[\"lb\"],\n",
    "    ec=\"black\",\n",
    "    color=\"slateblue\",\n",
    "    label=\"95% PI Lower Bound on Failure Count\",\n",
    "    alpha=0.3,\n",
    ")\n",
    "ax1.axvline(output_ds[\"lb\"].mean(), label=\"Expected 95% PI Lower Bound on Failure Count\")\n",
    "ax1.axvline(output_ds[\"ub\"].mean(), label=\"Expected 95% PI Upper Bound on Failure Count\")\n",
    "ax1.hist(\n",
    "    output_ds[\"ub\"],\n",
    "    ec=\"black\",\n",
    "    color=\"cyan\",\n",
    "    label=\"95% PI Upper Bound on Failure Count\",\n",
    "    bins=20,\n",
    "    alpha=0.3,\n",
    ")\n",
    "ax1.hist(\n",
    "    output_ds[\"expected\"], ec=\"black\", color=\"red\", label=\"Expected Count of Failures\", bins=20\n",
    ")\n",
    "ax1.set_title(\"Uncertainty in the Posterior Prediction Interval of Failure Counts\", fontsize=20)\n",
    "ax1.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a421478",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax2 = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "ax2.set_title(\"Expected Costs Distribution(s)  \\nbased on implied Failure counts\", fontsize=20)\n",
    "ax2.hist(\n",
    "    cost_func(output_ds[\"expected\"], 2),\n",
    "    label=\"Cost(failures,2)\",\n",
    "    color=\"cyan\",\n",
    "    alpha=0.3,\n",
    "    ec=\"black\",\n",
    "    bins=20,\n",
    ")\n",
    "ax2.hist(\n",
    "    cost_func(output_ds[\"expected\"], 2.1),\n",
    "    label=\"Cost(failures,2.1)\",\n",
    "    color=\"red\",\n",
    "    alpha=0.5,\n",
    "    ec=\"black\",\n",
    "    bins=20,\n",
    ")\n",
    "ax2.set_xlabel(\"$ cost\")\n",
    "ax2.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba92815",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c4e2d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbb790d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01ff7aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039285b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
